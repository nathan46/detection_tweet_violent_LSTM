{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bravo gars'\n",
      " 'félicitation 2 fois encore mieux jaimerai être maman aussi chanceux fille falloir patience beaucoup chance etpeut être quaprès concert'\n",
      " 'sioniste marcel connard trône train chier noter haut orgueil complexe supériorité rien plus affligeant monde depuis l  invention bêtis']\n",
      "Number of tweet : 104068\n"
     ]
    }
   ],
   "source": [
    "# read data from text files\n",
    "with open('cleanData.csv','r') as f:\n",
    "    tweet = f.read()\n",
    "\n",
    "#mettre minuscule\n",
    "tweet = tweet.lower()\n",
    "\n",
    "punctuation = \"!\\\"#$%&'()*+-./:;<=>?@[\\]^_`{|}~’\"\n",
    "\n",
    "all_tweet = ''.join([c for c in tweet if c not in punctuation])\n",
    "\n",
    "#print(all_tweet[:50])\n",
    "\n",
    "tweet_split = all_tweet.split('\\n') \n",
    "label_split = all_tweet.split('\\n') \n",
    "\n",
    "# separer tweet et label\n",
    "for index,entry in enumerate(tweet_split):\n",
    "    #print(index)\n",
    "    if(index != 0):\n",
    "        x = entry.split(',')\n",
    "        if(len(x) != 1):\n",
    "            tweet_split[index-1] = x[0]\n",
    "            label_split[index-1] = x[1]\n",
    "\n",
    "tweet_split = np.array(tweet_split)   \n",
    "label_split = np.array(label_split)  \n",
    "print (tweet_split[:3])\n",
    "print ('Number of tweet :', len(tweet_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.seed(354)\n",
    "\n",
    "indices = np.arange(tweet_split.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "tweet_split = tweet_split[indices]\n",
    "label_split = label_split[indices]\n",
    "\n",
    "#reduire taille echantillon\n",
    "tweet_split = tweet_split[:70000]\n",
    "\n",
    "#print (tweet_split[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index mapping\n",
    "from collections import Counter\n",
    "\n",
    "all_tweet2 = ' '.join(tweet_split)\n",
    "# create a list of words\n",
    "words = all_tweet2.split()\n",
    "\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "\n",
    "#print(sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "print (type(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[573, 10, 8605], [12, 139], [92, 10]]\n"
     ]
    }
   ],
   "source": [
    "tweet_int = []\n",
    "for tweet in tweet_split:\n",
    "    r = [vocab_to_int[w] for w in tweet.split()]\n",
    "    tweet_int.append(r)\n",
    "print (tweet_int[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49868355306145984\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#encode label\n",
    "encoded_labels = [1 if label =='1' else 0 for label in label_split]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "\n",
    "print(sum(encoded_labels)/len(encoded_labels))\n",
    "\n",
    "print(type(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    70000.000000\n",
       "mean        10.944543\n",
       "std          8.511726\n",
       "min          0.000000\n",
       "25%          4.000000\n",
       "50%          9.000000\n",
       "75%         16.000000\n",
       "max        267.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyse tweet length\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tweet_len = [len(x) for x in tweet_int]\n",
    "pd.Series(tweet_len).hist()\n",
    "plt.show()\n",
    "\n",
    "pd.Series(tweet_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD7CAYAAACG50QgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAARhElEQVR4nO3df4xlZX3H8fdH1h9ILaDoFBfSJXXVoFuVTBBr00yl4orG5Q9/YKwulmb/wda2myjapKT+aDCttdoa7Ea2rMaKhGogiuKKToxJUcAfIKBlqyi7BVFXaRejduy3f9xn2RmccWZn5t477PN+JZN7znOee85zvjv7ueeee+6ZVBWSpD48bNwDkCSNjqEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRJYV+kjuT3JLkq0lubG2PTbI7yR3t8fjWniTvSbInyc1JTpu1nq2t/x1Jtg5nlyRJCzmcI/3fr6pnVtVkm78QuK6qNgLXtXmAFwIb28824BIYvEgAFwHPBk4HLjr4QiFJGo11K3juFmCqTe8CpoE3tvYP1OBbX9cnOS7Jia3v7qraD5BkN7AZ+PBCGzjhhBNqw4YNKxji+N1///0cc8wx4x7GmmE95rIeh1iLuVZSj5tuuukHVfX4+ZYtNfQL+HSSAv65qnYAE1V1d1t+DzDRptcDd8167t7WtlD7gjZs2MCNN964xCGuTdPT00xNTY17GGuG9ZjLehxiLeZaST2SfGehZUsN/d+tqn1JngDsTvKN2QurqtoLwool2cbgtBATExNMT0+vxmrH5sCBAw/5fVhN1mMu63GItZhrWPVYUuhX1b72eG+SjzE4J/+9JCdW1d3t9M29rfs+4ORZTz+pte3j0Omgg+3T82xrB7ADYHJysh7qr/wevcxlPeayHodYi7mGVY9FP8hNckySxxycBs4Cvg5cDRy8AmcrcFWbvhp4TbuK5wzgvnYa6FrgrCTHtw9wz2ptkqQRWcqR/gTwsSQH+/9rVX0qyQ3AFUnOB74DvLz1vwY4G9gD/AR4LUBV7U/yVuCG1u8tBz/UlSSNxqKhX1XfAp4xT/sPgTPnaS/gggXWtRPYefjDlCStBr+RK0kdMfQlqSOGviR1xNCXpI6s5DYMa96GCz8xlu3eefGLxrJdSVqMR/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6sgRfcO1cZl9o7ftm2Y4b0Q3fvNGb5IW45G+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0JakjSw79JEcl+UqSj7f5U5J8McmeJB9J8ojW/sg2v6ct3zBrHW9q7d9M8oLV3hlJ0q92OH856/XA7cCvt/l3AO+qqsuTvA84H7ikPf6oqp6U5NzW7xVJTgXOBZ4GPBH4TJInV9UvVmlfurdhRH+haz7+1S7poWFJR/pJTgJeBLy/zQd4HnBl67ILOKdNb2nztOVntv5bgMur6mdV9W1gD3D6auyEJGlplnp65x+ANwD/1+YfB/y4qmba/F5gfZteD9wF0Jbf1/o/0D7PcyRJI7Do6Z0kLwburaqbkkwNe0BJtgHbACYmJpienl72urZvmlm805BNHL02xjFsS/13OnDgwIr+TY801uMQazHXsOqxlHP6zwVekuRs4FEMzum/Gzguybp2NH8SsK/13wecDOxNsg44FvjhrPaDZj/nAVW1A9gBMDk5WVNTU8vYrYHzxniO+6Dtm2Z45y2H89HJQ9Odr5paUr/p6WlW8m96pLEeh1iLuYZVj0VP71TVm6rqpKrawOCD2M9W1auAzwEvbd22Ale16avbPG35Z6uqWvu57eqeU4CNwJdWbU8kSYtaySHoG4HLk7wN+ApwaWu/FPhgkj3AfgYvFFTVrUmuAG4DZoALvHJHkkbrsEK/qqaB6Tb9Lea5+qaqfgq8bIHnvx14++EOUpK0OvxGriR1xNCXpI4c+ZeVaCSW+m3g7ZtmVvWqKr8JLB0ej/QlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOrJu3AOQVmLDhZ8Y27bvvPhFY9u2tFwe6UtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyKKhn+RRSb6U5GtJbk3y1639lCRfTLInyUeSPKK1P7LN72nLN8xa15ta+zeTvGBYOyVJmt9S7r3zM+B5VXUgycOBLyT5JPAXwLuq6vIk7wPOBy5pjz+qqiclORd4B/CKJKcC5wJPA54IfCbJk6vqF0PYL2noVuO+P9s3zXDeYa7He/5oJRY90q+BA2324e2ngOcBV7b2XcA5bXpLm6ctPzNJWvvlVfWzqvo2sAc4fVX2QpK0JEu6y2aSo4CbgCcB7wX+E/hxVc20LnuB9W16PXAXQFXNJLkPeFxrv37Wamc/Z/a2tgHbACYmJpienj68PZpl+6aZxTsN2cTRa2Mca4X1mGs59VjJ/4m17MCBA0fsvi3HsOqxpNBvp2CemeQ44GPAU1d9JIe2tQPYATA5OVlTU1PLXtfhvm0ehu2bZnjnLd7B+iDrMddy6nHnq6aGM5gxm56eZiX/3480w6rHYV29U1U/Bj4HPAc4LsnB39aTgH1teh9wMkBbfizww9nt8zxHkjQCS7l65/HtCJ8kRwPPB25nEP4vbd22Ale16avbPG35Z6uqWvu57eqeU4CNwJdWa0ckSYtbyvvKE4Fd7bz+w4ArqurjSW4DLk/yNuArwKWt/6XAB5PsAfYzuGKHqro1yRXAbcAMcIFX7kjSaC0a+lV1M/Csedq/xTxX31TVT4GXLbCutwNvP/xhSpJWg9/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXEO19JDzGrcR//5fJe/g99HulLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjnjJpqQlG+blots3zSz4d629VHT1eKQvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xHvvSFrzxvUnIo/Ee/54pC9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6smjoJzk5yeeS3Jbk1iSvb+2PTbI7yR3t8fjWniTvSbInyc1JTpu1rq2t/x1Jtg5vtyRJ81nKkf4MsL2qTgXOAC5IcipwIXBdVW0ErmvzAC8ENrafbcAlMHiRAC4Cng2cDlx08IVCkjQai4Z+Vd1dVV9u0/8D3A6sB7YAu1q3XcA5bXoL8IEauB44LsmJwAuA3VW1v6p+BOwGNq/q3kiSfqXD+kZukg3As4AvAhNVdXdbdA8w0abXA3fNetre1rZQ+4O3sY3BOwQmJiaYnp4+nCHOsX3TzLKfu1omjl4b41grrMdc1uOQtViLf/zQVWPb9inHHrWi/FvIkkM/ya8B/wb8WVX9d5IHllVVJanVGFBV7QB2AExOTtbU1NSy13XemL66Pdv2TTO88xbvdnGQ9ZjLehxiLea6bPMxrCT/FrKkq3eSPJxB4H+oqj7amr/XTtvQHu9t7fuAk2c9/aTWtlC7JGlElnL1ToBLgdur6u9nLboaOHgFzlbgqlntr2lX8ZwB3NdOA10LnJXk+PYB7lmtTZI0Ikt5L/Vc4NXALUm+2treDFwMXJHkfOA7wMvbsmuAs4E9wE+A1wJU1f4kbwVuaP3eUlX7V2UvJElLsmjoV9UXgCyw+Mx5+hdwwQLr2gnsPJwBSpJWj9/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRRUM/yc4k9yb5+qy2xybZneSO9nh8a0+S9yTZk+TmJKfNes7W1v+OJFuHszuSpF9lKUf6lwGbH9R2IXBdVW0ErmvzAC8ENrafbcAlMHiRAC4Cng2cDlx08IVCkjQ6i4Z+VX0e2P+g5i3Arja9CzhnVvsHauB64LgkJwIvAHZX1f6q+hGwm19+IZEkDdlyz+lPVNXdbfoeYKJNrwfumtVvb2tbqF2SNELrVrqCqqoktRqDAUiyjcGpISYmJpienl72urZvmlmlUS3fxNFrYxxrhfWYy3ocYi3mOnDgwIrybyHLDf3vJTmxqu5up2/ube37gJNn9Tupte0Dph7UPj3fiqtqB7ADYHJysqampubrtiTnXfiJZT93tWzfNMM7b1nxa+sRw3rMZT0OsRZzXbb5GFaSfwtZ7umdq4GDV+BsBa6a1f6adhXPGcB97TTQtcBZSY5vH+Ce1dokSSO06Mtqkg8zOEo/IcleBlfhXAxckeR84DvAy1v3a4CzgT3AT4DXAlTV/iRvBW5o/d5SVQ/+cFiSNGSLhn5VvXKBRWfO07eACxZYz05g52GNTpK0qvxGriR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOjDz0k2xO8s0ke5JcOOrtS1LPRhr6SY4C3gu8EDgVeGWSU0c5Bknq2aiP9E8H9lTVt6rq58DlwJYRj0GSujXq0F8P3DVrfm9rkySNQKpqdBtLXgpsrqo/bvOvBp5dVa+b1WcbsK3NPgX45sgGOBwnAD8Y9yDWEOsxl/U4xFrMtZJ6/GZVPX6+BeuWP55l2QecPGv+pNb2gKraAewY5aCGKcmNVTU57nGsFdZjLutxiLWYa1j1GPXpnRuAjUlOSfII4Fzg6hGPQZK6NdIj/aqaSfI64FrgKGBnVd06yjFIUs9GfXqHqroGuGbU2x2jI+ZU1SqxHnNZj0OsxVxDqcdIP8iVJI2Xt2GQpI4Y+kOU5LgkVyb5RpLbkzxn3GMalyR/nuTWJF9P8uEkjxr3mEYpyc4k9yb5+qy2xybZneSO9nj8OMc4SgvU42/b/5Wbk3wsyXHjHOMozVePWcu2J6kkJ6zGtgz94Xo38KmqeirwDOD2MY9nLJKsB/4UmKyqpzP4EP/c8Y5q5C4DNj+o7ULguqraCFzX5ntxGb9cj93A06vqt4H/AN406kGN0WX8cj1IcjJwFvDd1dqQoT8kSY4Ffg+4FKCqfl5VPx7vqMZqHXB0knXAo4H/GvN4RqqqPg/sf1DzFmBXm94FnDPSQY3RfPWoqk9X1UybvZ7B93i6sMDvB8C7gDcAq/bhq6E/PKcA3wf+JclXkrw/yTHjHtQ4VNU+4O8YHK3cDdxXVZ8e76jWhImqurtN3wNMjHMwa8wfAZ8c9yDGKckWYF9VfW0112voD8864DTgkqp6FnA/fb19f0A7V72FwQvhE4FjkvzheEe1ttTgMjovpQOS/CUwA3xo3GMZlySPBt4M/NVqr9vQH569wN6q+mKbv5LBi0CP/gD4dlV9v6r+F/go8DtjHtNa8L0kJwK0x3vHPJ6xS3Ie8GLgVdX39eS/xeAg6WtJ7mRwquvLSX5jpSs29Iekqu4B7krylNZ0JnDbGIc0Tt8Fzkjy6CRhUIsuP9R+kKuBrW16K3DVGMcydkk2Mzh//ZKq+sm4xzNOVXVLVT2hqjZU1QYGB5GntVxZEUN/uP4E+FCSm4FnAn8z5vGMRXu3cyXwZeAWBr93XX37MsmHgX8HnpJkb5LzgYuB5ye5g8G7oYvHOcZRWqAe/wQ8Btid5KtJ3jfWQY7QAvUYzrb6fgclSX3xSF+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8HnFHtYDDhzXkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove outliers\n",
    "tweet_int = [ tweet_int[i] for i, l in enumerate(tweet_len) if l>4 and l<15]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(tweet_len) if l> 4 and l<15]\n",
    "\n",
    "tweet_len = [len(x) for x in tweet_int]\n",
    "pd.Series(tweet_len).hist()\n",
    "plt.show()\n",
    "\n",
    "pd.Series(tweet_len).describe()\n",
    "\n",
    "for i in tweet_int:\n",
    "    i.reverse()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0  1514    22   459     1\n",
      "    122  2541 17376]\n",
      " [    0     0     0     0     0     0     0  1060   224   162    52   557\n",
      "     19 17377     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0    10   721\n",
      "   1114    18  5047]\n",
      " [    0     0     0     0     0     0     0     0     0     0 25593     9\n",
      "     11   341     1]\n",
      " [    0     0     0     0     0 25594   190    17   164    28    17    21\n",
      "     25   181     7]]\n"
     ]
    }
   ],
   "source": [
    "# paddin/trucate the remaining data\n",
    "def pad_features(tweet_int, seq_length):\n",
    "    ''' Return features of tweets_ints, where each tweet is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(tweet_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, tweet in enumerate(tweet_int):\n",
    "        tweet_len = len(tweet)\n",
    "        \n",
    "        if tweet_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-tweet_len))\n",
    "            new = zeroes+tweet        \n",
    "            \n",
    "        elif tweet_len > seq_length:\n",
    "            new = tweet[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = pad_features(tweet_int,15)\n",
    "print (features[:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   122  2541 17376]\n",
      " [    0     0     0 ...    19 17377     6]\n",
      " [    0     0     0 ...  1114    18  5047]\n",
      " ...\n",
      " [    0     0     0 ...  7727   304    82]\n",
      " [    0     0     0 ...  5235 21594   848]\n",
      " [    0   818    31 ...   130    27   604]]\n"
     ]
    }
   ],
   "source": [
    "# Training, validation, test datasets \n",
    "len_feat = len(tweet_int)\n",
    "split_frac = 0.8\n",
    "train_x = features[0:int(split_frac*len_feat)]\n",
    "train_y = np.array(encoded_labels[0:int(split_frac*len_feat)])\n",
    "\n",
    "remaining_x = features[int(split_frac*len_feat):]\n",
    "remaining_y = np.array(encoded_labels[int(split_frac*len_feat):])\n",
    "\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = np.array(remaining_y[0:int(len(remaining_y)*0.5)])\n",
    "\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = np.array(remaining_y[int(len(remaining_y)*0.5):])  \n",
    "\n",
    "print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 15])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,     0,     0,     0,     0,     0,     0,   991,\n",
      "            75, 17413,  6063,    31,    33],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,   512,  5064,\n",
      "          2859,  1418,    31,  5063,   442],\n",
      "        [    0,     0,     0,     0,     0,     0,    49, 13578,  1789,  2139,\n",
      "            78,   429,  2277,    38,    30],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0, 10640,\n",
      "         11074,  9651,   576,   208,    12],\n",
      "        [    0,     0,  1818,    80,  2485,  1956,   823, 54232,     1,  1667,\n",
      "          3878,   129,    34,  1249,   212],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,  1818,   324,\n",
      "            26,   585,  1005,   678, 33713],\n",
      "        [    0,     0,     0,  2223,   275,  2559,    48,   222,    78,    54,\n",
      "            10,    39,    50,     3,    92],\n",
      "        [    0,     0,     0,     0,     0, 28496, 28495,    11,  1053,     3,\n",
      "           180,     3,   399,   490, 28494],\n",
      "        [    0,     0,     0,     0,     0,   934,   539, 24058,  1239,    12,\n",
      "             8,   176,     6, 19581,  5181],\n",
      "        [    0,     0, 57921, 57920, 12422,   129,     6, 16045,  2454, 57919,\n",
      "           658,   106,   903,    82,    48],\n",
      "        [    0,     0, 14325,  2096,  2096,   795,    55,     6,  1813,  2048,\n",
      "          5977,   286,   309,   156,    33],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,   143,\n",
      "            14,     1,    11,   180,    95],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,   116, 10981,   111,\n",
      "           199,     4,    13,   284,     2],\n",
      "        [    0,     0,     0,     0,   256,   540,     1,  4271,    10,    76,\n",
      "            82,    35,     3,  3063,   329],\n",
      "        [    0,     0,     0,     0,     0,     0,    18,   326,   123,  8873,\n",
      "           252,    12,   670,  6800,   264],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,  7791, 24075,    16,\n",
      "         51240,    71,    53,  3680,     5],\n",
      "        [    0,     0,     0,     0,     0,     0, 40884,  1033,    67,   451,\n",
      "         16294,   315,  3632,     2, 40883],\n",
      "        [    0,     0,     0,     0,  1554,    10,    39,    23,  7261,     3,\n",
      "           728,   103,   108,  5735,  1030],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,   128,\n",
      "         22108,    63,     9,   151,    64],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,  8979,   678,\n",
      "             9,   651,  3583, 21168, 37693],\n",
      "        [    0,   485,    17,   580,  1811,     4,   123,  1055,    36, 22475,\n",
      "            27,    94,   116,  6896,   362],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,   778,   144,\n",
      "            41, 12509,   144,    61,   307],\n",
      "        [    0,     0,     0,     0,  1772,     8,    84,   104,  3382,   387,\n",
      "             2,   560,    19,  4440,  1212],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,   627,  1484,    16,\n",
      "           125,   185,   522,     3,     8],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "         39089,     2,   256, 11750,   318],\n",
      "        [    0,     0,     0,     0,     0,     0,  3423,   500,     6,   110,\n",
      "          2335, 56381,   500, 12259,  1308],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          1420,    68,   114, 11013,   814],\n",
      "        [    0,     0,     0,     0,    47,  4552, 32437, 10128,  4851,   718,\n",
      "           799,   387,     2,   195,     9],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          9819,    26,    37,   276,    15],\n",
      "        [    0,     0,     0,     0,     0,     0,   272,   369,     7,  4550,\n",
      "           339,     2,   174,  5710, 12949],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           144,    10,  3003,   294,   617],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "         50456,    31, 50455, 23937, 50454],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            68,   217,   709,   285,     9],\n",
      "        [    0,     0,     0,     0,     0,  8138,  9770,   989,     1,   170,\n",
      "           178,  3271,   490,   329,    16],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          2965,     9,    40,   164, 13354],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,  2350,\n",
      "         36522, 36521,    17,   553,     1],\n",
      "        [    0,     0,     0,     0,     0,     0, 30766,    12,   918,   121,\n",
      "          1367,   547,     1,   184,  1219],\n",
      "        [    0,     0,     0,   139,    56,  4728,  4727,     6,  7033,  2161,\n",
      "           663,   457,     7,  8239,  1425],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     8,\n",
      "            12,     4,    13,   204,   287],\n",
      "        [    0,     0,     0,     0,     0,     0,  3855,    90,   155,    10,\n",
      "            76,  1363,    54, 18512,    50],\n",
      "        [    0,     0,     0,     0,    26,     2,    60,    29,  5973,   174,\n",
      "            25,  4326,  8647,   253,    96],\n",
      "        [    0,     0,     0,     0,     0,  5456,   886,  1397, 33444,  4217,\n",
      "          6783,    25,     9,   105,     2],\n",
      "        [    0,     0,     0,     0,   206,  7771,   228,     3,  3786,  2282,\n",
      "           216,  4106,  1188,   751,    40],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     8,  2062,\n",
      "          3816,  5648,    55,    41,     5],\n",
      "        [    0,   385,   363,     4,    27,    31,   187,   236,     7,   165,\n",
      "          8616,  3773,   193,  3246,    44],\n",
      "        [    0,     0,     0,     0, 10182,  1134,   204,    17, 13018,   574,\n",
      "          1210,    30,   727,   460,   229],\n",
      "        [    0,     0,     0, 31990,  1839,  1839, 31989, 31988,    40,    41,\n",
      "           962,    37,  2355, 31987, 31986],\n",
      "        [    0,     0,     0,     0,     0,  1485,    29,   966,     9,    11,\n",
      "          5635,    18,     1,   192,     5],\n",
      "        [    0,     0,     0,     0,     0,     0,     0, 53676,   407,   155,\n",
      "           213,     2,   437,  1024,   778],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,   258,   279,\n",
      "             3,     8,    22,     2,   157]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
      "        0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Dataloaders and batching\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "#extends dimensions \n",
    "#train_data = np.expand_dims(train_data,axis=0)\n",
    "#valid_data = np.expand_dims(valid_data,axis=0)\n",
    "#test_data = np.expand_dims(test_data,axis=0)\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size,drop_last=True)\n",
    "\n",
    "\n",
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(68888, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5... Step: 100... Loss: 0.318234... Val Loss: 0.111605\n",
      "Epoch: 1/5... Step: 200... Loss: 0.154324... Val Loss: 0.096183\n",
      "Epoch: 1/5... Step: 300... Loss: 0.132048... Val Loss: 0.087603\n",
      "Epoch: 1/5... Step: 400... Loss: 0.040072... Val Loss: 0.092813\n",
      "Epoch: 1/5... Step: 500... Loss: 0.037403... Val Loss: 0.086540\n",
      "Epoch: 2/5... Step: 600... Loss: 0.076495... Val Loss: 0.080431\n",
      "Epoch: 2/5... Step: 700... Loss: 0.015116... Val Loss: 0.094441\n",
      "Epoch: 2/5... Step: 800... Loss: 0.028375... Val Loss: 0.079732\n",
      "Epoch: 2/5... Step: 900... Loss: 0.067484... Val Loss: 0.089470\n",
      "Epoch: 2/5... Step: 1000... Loss: 0.074311... Val Loss: 0.081677\n",
      "Epoch: 3/5... Step: 1100... Loss: 0.026583... Val Loss: 0.124457\n",
      "Epoch: 3/5... Step: 1200... Loss: 0.027192... Val Loss: 0.105301\n",
      "Epoch: 3/5... Step: 1300... Loss: 0.030637... Val Loss: 0.098996\n",
      "Epoch: 3/5... Step: 1400... Loss: 0.029730... Val Loss: 0.104819\n",
      "Epoch: 3/5... Step: 1500... Loss: 0.078280... Val Loss: 0.085813\n",
      "Epoch: 4/5... Step: 1600... Loss: 0.018626... Val Loss: 0.196910\n",
      "Epoch: 4/5... Step: 1700... Loss: 0.055507... Val Loss: 0.109001\n",
      "Epoch: 4/5... Step: 1800... Loss: 0.078976... Val Loss: 0.098244\n",
      "Epoch: 4/5... Step: 1900... Loss: 0.070757... Val Loss: 0.113850\n",
      "Epoch: 4/5... Step: 2000... Loss: 0.040263... Val Loss: 0.093894\n",
      "Epoch: 4/5... Step: 2100... Loss: 0.054370... Val Loss: 0.108536\n",
      "Epoch: 5/5... Step: 2200... Loss: 0.008711... Val Loss: 0.121938\n",
      "Epoch: 5/5... Step: 2300... Loss: 0.004303... Val Loss: 0.130680\n",
      "Epoch: 5/5... Step: 2400... Loss: 0.078185... Val Loss: 0.102912\n",
      "Epoch: 5/5... Step: 2500... Loss: 0.136209... Val Loss: 0.101963\n",
      "Epoch: 5/5... Step: 2600... Loss: 0.144511... Val Loss: 0.105901\n"
     ]
    }
   ],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.01\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 5 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=6 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "            \n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                #inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.122\n",
      "Test accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    #print(type(output))\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ponctuation = \"!\\\"#$%&'()*+-./:;<=>?@[\\]^_`{|}~’\"\n",
    "\n",
    "def preprocess(tweet, vocab_to_int):\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #enlever ponctuation\n",
    "    tweet = ''.join([c for c in tweet if c not in punctuation])\n",
    "    \n",
    "    #enlever les liens\n",
    "    indice = tweet.find(\"http\")\n",
    "    if indice != -1:\n",
    "        tweet = tweet[:indice]    \n",
    "\n",
    "    #remove stopwords\n",
    "    x = tweet.split()\n",
    "    text_final = \"\"\n",
    "    \n",
    "    for word in x:\n",
    "        if word not in stopwords.words('french'):\n",
    "            text_final = text_final+\" \"+word\n",
    "         \n",
    "    tweet = text_final \n",
    "    \n",
    "    #lemmatisation\n",
    "    doc = nlp(tweet)\n",
    "    text_final = \"\"\n",
    "    for token in doc:\n",
    "        text_final = text_final+\" \"+token.lemma_\n",
    "        \n",
    "    tweet = text_final.strip() \n",
    "\n",
    "    word_list = tweet.split()\n",
    "    num_list = []\n",
    "    #list of reviews\n",
    "    #though it contains only one review as of now\n",
    "    tweets_int = []\n",
    "    for word in word_list:\n",
    "        if word in vocab_to_int.keys():\n",
    "            num_list.append(vocab_to_int[word])\n",
    "    tweets_int.append(num_list)\n",
    "    return tweets_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, test_tweet, sequence_length=30):\n",
    "    ''' Prints out whether a give review is predicted to be \n",
    "        positive or negative in sentiment, using a trained model.\n",
    "        \n",
    "        params:\n",
    "        net - A trained net \n",
    "        test_review - a review made of normal text and punctuation\n",
    "        sequence_length - the padded length of a review\n",
    "        '''\n",
    "\n",
    "    #change the reviews to sequence of integers\n",
    "    int_rev = preprocess(test_tweet, vocab_to_int)\n",
    "    #pad the reviews as per the sequence length of the feature\n",
    "    features = pad_features(int_rev, seq_length=seq_length)\n",
    "    \n",
    "    #changing the features to PyTorch tensor\n",
    "    features = torch.from_numpy(features)\n",
    "    \n",
    "    #pass the features to the model to get prediction\n",
    "    net.eval()\n",
    "    val_h = net.init_hidden(1)\n",
    "    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        features = features.cuda()\n",
    "\n",
    "    output, val_h = net(features, val_h)\n",
    "    \n",
    "    #rounding the output to nearest 0 or 1\n",
    "    #pred = torch.round(output)\n",
    "    \n",
    "    #mapping the numeric values to postive or negative\n",
    "    #output = [\"Positive\" if pred.item() == 0 else \"Negative\"]\n",
    "    \n",
    "    # print custom response based on whether test_review is pos/neg\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, je suis heureux aujourd'hui : 0.0007397727458737791\n",
      "Merci beaucoup pour votre soutient c'est tres gentil : 0.1904749572277069\n",
      "\n",
      "Les violeurs comme toi merite d'aller en taule fils de pute : 0.9991791844367981\n",
      "T'etonne pas si on t'attend a la sortie du college : 0.09213431924581528\n",
      "\n",
      "c'est le seul Raoul en qui j'ai confiance : 0.7078825235366821\n"
     ]
    }
   ],
   "source": [
    "seq_length=15\n",
    "my_tweet_neutre = \"Bonjour, je suis heureux aujourd'hui\"\n",
    "my_tweet_neutre2 = \"Merci beaucoup pour votre soutient c'est tres gentil\"\n",
    "\n",
    "my_tweet_mauvais = \"Les violeurs comme toi merite d'aller en taule fils de pute\"\n",
    "my_tweet_mauvais2 = \"T'etonne pas si on t'attend a la sortie du college\"\n",
    "\n",
    "my_tweet_test = \"c'est le seul Raoul en qui j'ai confiance\"\n",
    "\n",
    "print(my_tweet_neutre + \" : \" + str(predict(net, my_tweet_neutre, seq_length)))\n",
    "print(my_tweet_neutre2 + \" : \" + str(predict(net, my_tweet_neutre2, seq_length)))\n",
    "print()\n",
    "print(my_tweet_mauvais + \" : \" + str(predict(net, my_tweet_mauvais, seq_length)))\n",
    "print(my_tweet_mauvais2 + \" : \" + str(predict(net, my_tweet_mauvais2, seq_length)))\n",
    "print()\n",
    "print(my_tweet_test + \" : \" + str(predict(net, my_tweet_test, seq_length)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def saveModel(net, name):\n",
    "    torch.save(net.state_dict(), name+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModel(name):\n",
    "    vocab_size = 30916 # +1 for the 0 padding\n",
    "    output_size = 1\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 256\n",
    "    n_layers = 2\n",
    "    model = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "    model.load_state_dict(torch.load(name+'.pth'))\n",
    "    model.eval()\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        model.cuda()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveModel(net, 'model_70k_3c_50b_5e_len15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadModel('model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import json\n",
    "\n",
    "with codecs.open('vocab_to_int-70k_3c_50b_5e_len15.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(vocab_to_int, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('vocab_to_int.json') as f:\n",
    "    data = json.load(f)\n",
    "   \n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with codecs.open('tweet_int.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(tweet_int, f, ensure_ascii=False)\n",
    "    \n",
    "with open('tweet_int.json') as f:\n",
    "    data2 = json.load(f)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
